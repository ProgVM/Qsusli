import re
import requests
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import json
import os
import urllib.parse
import urllib.request
import hashlib
from PIL import Image, ImageFilter, ImageEnhance, ImageDraw
from collections import defaultdict
import time
import random
import math
from db import DB

db = DB()

class BPE:
    def __init__(self):
        self.vocab = {}
        self.merges = []

    def get_stats(self, tokens):
        pairs = defaultdict(int)
        for token, freq in tokens.items():
            symbols = token.split()
            for i in range(len(symbols)-1):
                pairs[(symbols[i], symbols[i+1])] += freq
        return pairs

    def merge_vocab(self, pair, tokens):
        new_tokens = {}
        bigram = ' '.join(pair)
        pattern = re.compile(r'(?<!\S)' + re.escape(bigram) + r'(?!\S)')
        for token, freq in tokens.items():
            new_token = pattern.sub(''.join(pair), token)
            new_tokens[new_token] = freq + new_tokens.get(new_token, 0)
        return new_tokens

    def learn_bpe(self, corpus, max_merges=3000, min_freq=10):
        tokens = {}
        # –°–æ–±–∏—Ä–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –∏–∑ –∫–æ—Ä–ø—É—Å–∞
        base_chars = set()
        for word in corpus:
            base_chars.update(list(word))
            token = ' '.join(list(word) + ['</w>'])
            tokens[token] = tokens.get(token, 0) + 1

        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã —è–≤–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä—å
        self.vocab = {ch: idx for idx, ch in enumerate(sorted(base_chars))}
        self.vocab.update({'<unk>': len(self.vocab), '<pad>': len(self.vocab), '</w>': len(self.vocab), '<space>': len(self.vocab)})

        for i in range(max_merges):
            pairs = self.get_stats(tokens)
            if not pairs:
                break
            best_pair, best_freq = max(pairs.items(), key=lambda x: x[1])
            if best_freq < min_freq:
                break
            tokens = self.merge_vocab(best_pair, tokens)
            self.merges.append(best_pair)
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ª–æ–≤–∞—Ä—å –Ω–æ–≤—ã–º–∏ –ø–æ–¥—Å–ª–æ–≤–∞–º–∏
            new_token = ''.join(best_pair)
            if new_token not in self.vocab:
                self.vocab[new_token] = len(self.vocab)

    def encode(self, text):
        if not text or not text.strip():
            return ['<unk>']

        text = text.lower().strip()
        words = text.split()
        all_tokens = []

        for i, word in enumerate(words):
            if i > 0:
                all_tokens.append('<space>')
            word_tokens = list(word) + ['</w>']

            for merge in self.merges:
                j = 0
                while j < len(word_tokens) -1:
                    if word_tokens[j] == merge[0] and word_tokens[j+1] == merge[1]:
                        word_tokens[j:j+2] = [''.join(merge)]
                    else:
                        j +=1

            all_tokens.extend(word_tokens)

        result = []
        for token in all_tokens:
            if token in self.vocab:
                result.append(token)
            else:
                result.append('<unk>')

        return result

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=512):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :].to(x.device)
        return self.dropout(x)

class Transformer(nn.Module):
    def __init__(self, vocab_size, embed_size=256, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.1, max_len=512):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.pos_encoder = PositionalEncoding(embed_size, dropout, max_len)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_size,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(embed_size, vocab_size)
        self.max_len = max_len

    def forward(self, src, src_key_padding_mask=None):
        seq_len = src.size(1)
        if seq_len > self.max_len:
            src = src[:, :self.max_len]
            seq_len = self.max_len
        emb = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)
        emb = self.pos_encoder(emb)
        out = self.transformer_encoder(emb, src_key_padding_mask=src_key_padding_mask)
        out = self.fc_out(out)
        return out

class GlobalWorkspace:
    def __init__(self):
        self.semantic_info = None
        self.context_keywords = []
        self.user_ratings = []
        self.dialog_history = []
        self.current_prompt = None

    def update_semantic(self, semantic_data):
        self.semantic_info = semantic_data

    def update_keywords(self, keywords):
        self.context_keywords = keywords

    def add_user_rating(self, rating):
        self.user_ratings.append(rating)
        if len(self.user_ratings) > 50:
            self.user_ratings = self.user_ratings[-50:]

    def add_dialog_pair(self, user_text, bot_response):
        self.dialog_history.append((user_text, bot_response))
        if len(self.dialog_history) > 100:
            self.dialog_history = self.dialog_history[-100:]

    def set_current_prompt(self, prompt):
        self.current_prompt = prompt

    def summarize_state(self):
        return {
            'primary_category': self.semantic_info.get('primary_category') if self.semantic_info else None,
            'keywords': self.context_keywords,
            'recent_ratings_avg': (sum(self.user_ratings)/len(self.user_ratings)) if self.user_ratings else None,
            'last_user_message': self.dialog_history[-1][0] if self.dialog_history else None
        }

    def meta_learning_update(self, generation_params=None, success_metric=None):
        if generation_params is None:
            generation_params = {}

        avg_rating = self.user_ratings[-10:]
        if avg_rating:
            avg = sum(avg_rating)/len(avg_rating)
            if avg < 0.5:
                generation_params['temperature'] = max(0.3, generation_params.get('temperature', 0.7) - 0.1)
            elif avg > 0.8:
                generation_params['temperature'] = min(1.0, generation_params.get('temperature', 0.7) + 0.1)

        return generation_params

def tokens_to_indices(tokens, vocab):
    unk_idx = vocab.get('<unk>', 0)
    return [vocab.get(token, unk_idx) for token in tokens if token]

def pad_sequences(sequences, pad_value=0, max_len=None):
    if max_len is None:
        max_len = max(len(seq) for seq in sequences)
    padded_seqs = []
    for seq in sequences:
        padded = seq + [pad_value] * (max_len - len(seq))
        padded_seqs.append(padded)
    return padded_seqs

def prepare_sequences(sentences, bpe):
    inputs = []
    targets = []
    for sentence in sentences:
        words = sentence.lower().split()
        tokens = []
        for w in words:
            tokens.extend(bpe.encode(w))
        if len(tokens) < 2:
            continue
        inputs.append(tokens_to_indices(tokens[:-1], bpe.vocab))
        targets.append(tokens_to_indices(tokens[1:], bpe.vocab))
    if not inputs:
        return None, None, None
    max_len = max(max(len(seq) for seq in inputs), max(len(seq) for seq in targets))
    inputs_padded = pad_sequences(inputs, pad_value=bpe.vocab.get('<pad>', 0), max_len=max_len)
    targets_padded = pad_sequences(targets, pad_value=bpe.vocab.get('<pad>', 0), max_len=max_len)

    padding_mask = []
    for seq in inputs_padded:
        mask = [token == bpe.vocab.get('<pad>', 0) for token in seq]
        padding_mask.append(mask)

    input_tensor = torch.tensor(inputs_padded, dtype=torch.long)
    target_tensor = torch.tensor(targets_padded, dtype=torch.long)
    padding_mask_tensor = torch.tensor(padding_mask)  # bool –∏–ª–∏ byte

    return input_tensor, target_tensor, padding_mask_tensor

def split_text_to_sentences(text, max_len=50):
    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    sentences = re.split(r'[.!?]+', text)
    filtered = []
    for sent in sentences:
        sent = sent.strip()
        if 5 <= len(sent.split()) <= max_len:
            filtered.append(sent)

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
    if not filtered:
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            if 1 <= len(line.split()) <= max_len:
                filtered.append(line)

    return filtered

def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
    assert logits.dim() == 1  # logits - –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä
    top_k = min(top_k, logits.size(-1))  # Safety check

    if top_k > 0:
        indices_to_remove = logits < torch.topk(logits, top_k)[0][-1]
        logits[indices_to_remove] = filter_value

    if top_p > 0.0:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

        # –£–¥–∞–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã —Å –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –≤—ã—à–µ top_p
        sorted_indices_to_remove = cumulative_probs > top_p
        # –°–¥–≤–∏–≥–∞–µ–º –Ω–∞ 1, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0

        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        logits[indices_to_remove] = filter_value

    return logits

def generate_text(model, start_tokens, max_len, inv_vocab, temperature=1.0, top_k=50, top_p=0.9, bpe_vocab=None):
    model.eval()
    generated = start_tokens[:]
    input_seq = torch.tensor([start_tokens])
    with torch.no_grad():
        for _ in range(max_len):
            output = model(input_seq)
            logits = output[0, -1] / temperature
            logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)

            if bpe_vocab is not None:
                space_token_idx = bpe_vocab.get('<space>')
                if space_token_idx is not None:
                    prob_space = torch.softmax(logits, dim=0)[space_token_idx].item()
                    print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–±–µ–ª–∞: {prob_space:.4f}")

            probs = torch.softmax(logits, dim=0)
            if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():
                print("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞.")
                break
            next_token = torch.multinomial(probs, 1).item()
            generated.append(next_token)
            input_seq = torch.tensor([generated])
            if inv_vocab[next_token].endswith('</w>'):
                break
    return [inv_vocab[idx] for idx in generated]

def train_new_model():
    corpus = [
        # English greetings and common phrases
        "hello world", "hi there", "hey you", "greetings friend", "good morning everyone",
        "good evening all", "how are you today", "what's up", "how is it going",
        "nice to meet you", "thank you very much", "thanks a lot", "you're welcome",
        "sorry about that", "excuse me please", "goodbye for now", "see you later",
        "take care", "have a nice day", "yes please", "no thanks", "maybe later",
        "I don't know", "can you help me", "what is your name", "tell me a joke",
        "how old are you", "where are you from", "what do you do", "I love programming",
        "Python is great", "let's learn AI", "machine learning is fun",
        "artificial intelligence is fascinating", "neural networks are powerful",
        "deep learning techniques", "have a good evening", "see you soon",
        "please wait a moment", "thank you for your help", "sorry for the delay",
        "can you explain that", "what time is it", "how can I assist you", "good night",
        "happy birthday", "congratulations", "let's start", "welcome aboard",
        "have a great weekend", "enjoy your meal", "best wishes", "all the best",

        # Russian greetings and common phrases
        "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π –¥—Ä—É–≥", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ –≤—Å–µ–º", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä", "–∫–∞–∫ –¥–µ–ª–∞ —Å–µ–≥–æ–¥–Ω—è",
        "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "—Ä–∞–¥ —Ç–µ–±—è –≤–∏–¥–µ—Ç—å", "—Å–ø–∞—Å–∏–±–æ –±–æ–ª—å—à–æ–µ", "–ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "–∏–∑–≤–∏–Ω–∏ –∑–∞ —ç—Ç–æ",
        "–ø—Ä–æ—Å—Ç–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è", "—É–≤–∏–¥–∏–º—Å—è –ø–æ–∑–∂–µ", "–±–µ—Ä–µ–≥–∏ —Å–µ–±—è", "—Ö–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è",
        "–¥–∞, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞", "–Ω–µ—Ç, —Å–ø–∞—Å–∏–±–æ", "–º–æ–∂–µ—Ç –ø–æ–∑–∂–µ", "—è –Ω–µ –∑–Ω–∞—é", "—Ç—ã –º–æ–∂–µ—à—å –ø–æ–º–æ—á—å",
        "–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç", "—Ä–∞—Å—Å–∫–∞–∂–∏ —à—É—Ç–∫—É", "—Å–∫–æ–ª—å–∫–æ —Ç–µ–±–µ –ª–µ—Ç", "–æ—Ç–∫—É–¥–∞ —Ç—ã", "—á–µ–º –∑–∞–Ω–∏–º–∞–µ—à—å—Å—è",
        "—è –ª—é–±–ª—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å", "Python –æ—Ç–ª–∏—á–Ω—ã–π —è–∑—ã–∫", "–¥–∞–≤–∞–π –∏–∑—É—á–∞—Ç—å –ò–ò", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ",
        "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —É–¥–∏–≤–∏—Ç–µ–ª–µ–Ω", "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –º–æ—â–Ω—ã–µ", "—Ç–µ—Ö–Ω–∏–∫–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
        "—Ö–æ—Ä–æ—à–µ–≥–æ –≤–µ—á–µ—Ä–∞", "–¥–æ —Å–∫–æ—Ä–æ–≥–æ", "–ø–æ–¥–æ–∂–¥–∏ –º–∏–Ω—É—Ç—É", "—Å–ø–∞—Å–∏–±–æ –∑–∞ –ø–æ–º–æ—â—å", "–∏–∑–≤–∏–Ω–∏ –∑–∞ –∑–∞–¥–µ—Ä–∂–∫—É",
        "–º–æ–∂–µ—à—å –æ–±—ä—è—Å–Ω–∏—Ç—å —ç—Ç–æ", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "—á–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å", "—Å–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏",
        "—Å –¥–Ω–µ–º —Ä–æ–∂–¥–µ–Ω–∏—è", "–ø–æ–∑–¥—Ä–∞–≤–ª—è—é", "–¥–∞–≤–∞–π –Ω–∞—á–Ω–µ–º", "–¥–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –Ω–∞ –±–æ—Ä—Ç",
        "—Ö–æ—Ä–æ—à–∏—Ö –≤—ã—Ö–æ–¥–Ω—ã—Ö", "–ø—Ä–∏—è—Ç–Ω–æ–≥–æ –∞–ø–ø–µ—Ç–∏—Ç–∞", "–ª—É—á—à–∏–µ –ø–æ–∂–µ–ª–∞–Ω–∏—è", "–≤—Å–µ–≥–æ –Ω–∞–∏–ª—É—á—à–µ–≥–æ",

        # English questions and info
        "what is the weather today", "how to cook pasta", "tell me a story",
        "what is the capital of France", "how do airplanes fly", "what is quantum physics",
        "define artificial intelligence", "explain machine learning", "who is Elon Musk",
        "latest news in technology", "how to learn programming", "tips for studying",
        "best movies of 2025", "how to stay healthy", "what is meditation",
        "history of the internet", "how to play chess", "what is blockchain",
        "how to improve memory", "what is climate change", "how to travel cheap",
        "best books to read", "how to start a business", "what is cryptocurrency",
        "how to bake a cake", "what is the meaning of life", "how to be happy",
        "tips for time management", "best programming languages", "how to learn Python",
        "what is deep learning", "how to build a website", "what is data science",
        "how to make coffee", "best places to visit", "how to learn English",
        "what is virtual reality", "how to write a resume", "how to meditate",
        "best exercises for abs", "how to lose weight", "what is AI ethics",
        "how to improve coding skills", "best podcasts to listen", "how to invest money",
        "what is natural language processing", "how to start blogging", "best hiking trails",
        "how to learn guitar", "what is cloud computing", "how to create art",
        "best ways to relax", "how to speak in public", "what is 5G technology",

        # Russian questions and info
        "–∫–∞–∫–∞—è —Å–µ–≥–æ–¥–Ω—è –ø–æ–≥–æ–¥–∞", "–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –ø–∞—Å—Ç—É", "—Ä–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –∏—Å—Ç–æ—Ä–∏—é",
        "–∫–∞–∫–∞—è —Å—Ç–æ–ª–∏—Ü–∞ –§—Ä–∞–Ω—Ü–∏–∏", "–∫–∞–∫ –ª–µ—Ç–∞—é—Ç —Å–∞–º–æ–ª–µ—Ç—ã", "—á—Ç–æ —Ç–∞–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞",
        "–æ–ø—Ä–µ–¥–µ–ª–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–æ–±—ä—è—Å–Ω–∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π –ò–ª–æ–Ω –ú–∞—Å–∫",
        "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π", "–∫–∞–∫ —É—á–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "—Å–æ–≤–µ—Ç—ã –¥–ª—è —É—á–µ–±—ã",
        "–ª—É—á—à–∏–µ —Ñ–∏–ª—å–º—ã 2025 –≥–æ–¥–∞", "–∫–∞–∫ –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –∑–¥–æ—Ä–æ–≤—ã–º", "—á—Ç–æ —Ç–∞–∫–æ–µ –º–µ–¥–∏—Ç–∞—Ü–∏—è",
        "–∏—Å—Ç–æ—Ä–∏—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞", "–∫–∞–∫ –∏–≥—Ä–∞—Ç—å –≤ —à–∞—Ö–º–∞—Ç—ã", "—á—Ç–æ —Ç–∞–∫–æ–µ –±–ª–æ–∫—á–µ–π–Ω",
        "–∫–∞–∫ —É–ª—É—á—à–∏—Ç—å –ø–∞–º—è—Ç—å", "—á—Ç–æ —Ç–∞–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–ª–∏–º–∞—Ç–∞", "–∫–∞–∫ –ø—É—Ç–µ—à–µ—Å—Ç–≤–æ–≤–∞—Ç—å –¥–µ—à–µ–≤–æ",
        "–ª—É—á—à–∏–µ –∫–Ω–∏–≥–∏ –¥–ª—è —á—Ç–µ–Ω–∏—è", "–∫–∞–∫ –Ω–∞—á–∞—Ç—å –±–∏–∑–Ω–µ—Å", "—á—Ç–æ —Ç–∞–∫–æ–µ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞",
        "–∫–∞–∫ –∏—Å–ø–µ—á—å —Ç–æ—Ä—Ç", "–≤ —á–µ–º —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏", "–∫–∞–∫ –±—ã—Ç—å —Å—á–∞—Å—Ç–ª–∏–≤—ã–º",
        "—Å–æ–≤–µ—Ç—ã –ø–æ —Ç–∞–π–º-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç—É", "–ª—É—á—à–∏–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "–∫–∞–∫ —É—á–∏—Ç—å Python",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Å–∞–π—Ç", "—á—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—É–∫–∞ –æ –¥–∞–Ω–Ω—ã—Ö",
        "–∫–∞–∫ –ø—Ä–∏–≥–æ—Ç–æ–≤–∏—Ç—å –∫–æ—Ñ–µ", "–ª—É—á—à–∏–µ –º–µ—Å—Ç–∞ –¥–ª—è –ø–æ—Å–µ—â–µ–Ω–∏—è", "–∫–∞–∫ —É—á–∏—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–π",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª—å–Ω–æ—Å—Ç—å", "–∫–∞–∫ –Ω–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—é–º–µ", "–∫–∞–∫ –º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å",
        "–ª—É—á—à–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –¥–ª—è –ø—Ä–µ—Å—Å–∞", "–∫–∞–∫ –ø–æ—Ö—É–¥–µ—Ç—å", "—á—Ç–æ —Ç–∞–∫–æ–µ —ç—Ç–∏–∫–∞ –ò–ò",
        "–∫–∞–∫ —É–ª—É—á—à–∏—Ç—å –Ω–∞–≤—ã–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è", "–ª—É—á—à–∏–µ –ø–æ–¥–∫–∞—Å—Ç—ã –¥–ª—è –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è", "–∫–∞–∫ –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–µ–Ω—å–≥–∏",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "–∫–∞–∫ –Ω–∞—á–∞—Ç—å –±–ª–æ–≥", "–ª—É—á—à–∏–µ –º–∞—Ä—à—Ä—É—Ç—ã –¥–ª—è –ø–æ—Ö–æ–¥–æ–≤",
        "–∫–∞–∫ —É—á–∏—Ç—å –≥–∏—Ç–∞—Ä—É", "—á—Ç–æ —Ç–∞–∫–æ–µ –æ–±–ª–∞—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "–∫–∞–∫ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–æ",
        "–ª—É—á—à–∏–µ —Å–ø–æ—Å–æ–±—ã —Ä–∞—Å—Å–ª–∞–±–∏—Ç—å—Å—è", "–∫–∞–∫ –≥–æ–≤–æ—Ä–∏—Ç—å –ø—É–±–ª–∏—á–Ω–æ", "—á—Ç–æ —Ç–∞–∫–æ–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è 5G",

        # English emotions and moods
        "I am happy today", "feeling sad", "excited about the trip", "worried about exams",
        "love and friendship", "feeling tired", "full of energy", "need a break",

        # Russian emotions and moods
        "—è —Å–µ–≥–æ–¥–Ω—è —Å—á–∞—Å—Ç–ª–∏–≤", "—á—É–≤—Å—Ç–≤—É—é –≥—Ä—É—Å—Ç—å", "–≤–∑–≤–æ–ª–Ω–æ–≤–∞–Ω –ø–æ–µ–∑–¥–∫–æ–π", "–±–µ—Å–ø–æ–∫–æ—é—Å—å –∏–∑-–∑–∞ —ç–∫–∑–∞–º–µ–Ω–æ–≤",
        "–ª—é–±–æ–≤—å –∏ –¥—Ä—É–∂–±–∞", "—á—É–≤—Å—Ç–≤—É—é —É—Å—Ç–∞–ª–æ—Å—Ç—å", "–ø–æ–ª–æ–Ω —ç–Ω–µ—Ä–≥–∏–∏", "–Ω—É–∂–µ–Ω –ø–µ—Ä–µ—Ä—ã–≤",

        # English commands and phrases
        "open the door", "turn on the lights", "play some music", "set an alarm for 7 am",
        "call mom", "send an email", "what's the news", "tell me a joke",
        "translate this sentence", "find nearby restaurants", "book a taxi",
        "order pizza", "check the weather", "remind me to buy milk",
        "how to fix a bike", "what is the time", "where is the nearest bank",
        "how to learn cooking", "best programming tutorials", "latest tech trends",
        "how to meditate properly", "tips for good sleep", "how to reduce stress",
        "best travel destinations", "how to learn languages fast", "how to write poetry",
        "how to improve memory", "how to be productive", "healthy lifestyle tips",

        # Russian commands and phrases
        "–æ—Ç–∫—Ä–æ–π –¥–≤–µ—Ä—å", "–≤–∫–ª—é—á–∏ —Å–≤–µ—Ç", "–≤–∫–ª—é—á–∏ –º—É–∑—ã–∫—É", "—É—Å—Ç–∞–Ω–æ–≤–∏ –±—É–¥–∏–ª—å–Ω–∏–∫ –Ω–∞ 7 —É—Ç—Ä–∞",
        "–ø–æ–∑–≤–æ–Ω–∏ –º–∞–º–µ", "–æ—Ç–ø—Ä–∞–≤—å –ø–∏—Å—å–º–æ", "–∫–∞–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏", "—Ä–∞—Å—Å–∫–∞–∂–∏ —à—É—Ç–∫—É",
        "–ø–µ—Ä–µ–≤–µ–¥–∏ —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "–Ω–∞–π–¥–∏ –±–ª–∏–∂–∞–π—à–∏–µ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã", "–∑–∞–∫–∞–∂–∏ —Ç–∞–∫—Å–∏",
        "–∑–∞–∫–∞–∂–∏ –ø–∏—Ü—Ü—É", "–ø—Ä–æ–≤–µ—Ä—å –ø–æ–≥–æ–¥—É", "–Ω–∞–ø–æ–º–Ω–∏ –∫—É–ø–∏—Ç—å –º–æ–ª–æ–∫–æ",
        "–∫–∞–∫ –ø–æ—á–∏–Ω–∏—Ç—å –≤–µ–ª–æ—Å–∏–ø–µ–¥", "–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å", "–≥–¥–µ –±–ª–∏–∂–∞–π—à–∏–π –±–∞–Ω–∫",
        "–∫–∞–∫ –Ω–∞—É—á–∏—Ç—å—Å—è –≥–æ—Ç–æ–≤–∏—Ç—å", "–ª—É—á—à–∏–µ —É—Ä–æ–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π",
        "–∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –º–µ–¥–∏—Ç–∏—Ä–æ–≤–∞—Ç—å", "—Å–æ–≤–µ—Ç—ã –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ —Å–Ω–∞", "–∫–∞–∫ —Å–Ω–∏–∑–∏—Ç—å —Å—Ç—Ä–µ—Å—Å",
        "–ª—É—á—à–∏–µ —Ç—É—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è", "–∫–∞–∫ –±—ã—Å—Ç—Ä–æ —É—á–∏—Ç—å —è–∑—ã–∫–∏", "–∫–∞–∫ –ø–∏—Å–∞—Ç—å —Å—Ç–∏—Ö–∏",
        "–∫–∞–∫ —É–ª—É—á—à–∏—Ç—å –ø–∞–º—è—Ç—å", "–∫–∞–∫ –±—ã—Ç—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ã–º", "—Å–æ–≤–µ—Ç—ã –¥–ª—è –∑–¥–æ—Ä–æ–≤–æ–≥–æ –æ–±—Ä–∞–∑–∞ –∂–∏–∑–Ω–∏",

        # Additional English phrases with punctuation and spaces
        "Hello, how are you?", "What's your name?", "Can you help me, please?",
        "I don't know what to do.", "Let's meet at 5 pm.", "Thank you very much!",
        "Sorry, I was late.", "Good morning, everyone!", "Have a nice day!",
        "See you soon.", "Take care!", "How's the weather today?", "Is it raining?",
        "Where can I find a good restaurant?", "Tell me something interesting.",
        "Do you like music?", "I love programming in Python.", "Let's learn AI together.",
        "Can you explain machine learning?", "What is deep learning?", "How old are you?",
        "What time is it now?", "Please help me with my homework.", "Happy birthday!",
        "Congratulations on your achievement!", "Good luck!", "Best wishes to you.",

        # Additional Russian phrases with punctuation and spaces
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?", "–ö–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç?", "–ú–æ–∂–µ—à—å –ø–æ–º–æ—á—å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞?",
        "–Ø –Ω–µ –∑–Ω–∞—é, —á—Ç–æ –¥–µ–ª–∞—Ç—å.", "–î–∞–≤–∞–π –≤—Å—Ç—Ä–µ—Ç–∏–º—Å—è –≤ 5 –≤–µ—á–µ—Ä–∞.", "–ë–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ!",
        "–ò–∑–≤–∏–Ω–∏, —è –æ–ø–æ–∑–¥–∞–ª.", "–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ, –≤—Å–µ–º!", "–•–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è!",
        "–î–æ —Å–∫–æ—Ä–æ–≥–æ.", "–ë–µ—Ä–µ–≥–∏ —Å–µ–±—è!", "–ö–∞–∫–∞—è —Å–µ–≥–æ–¥–Ω—è –ø–æ–≥–æ–¥–∞?", "–ò–¥—ë—Ç –ª–∏ –¥–æ–∂–¥—å?",
        "–ì–¥–µ –Ω–∞–π—Ç–∏ —Ö–æ—Ä–æ—à–∏–π —Ä–µ—Å—Ç–æ—Ä–∞–Ω?", "–†–∞—Å—Å–∫–∞–∂–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ.",
        "–¢—ã –ª—é–±–∏—à—å –º—É–∑—ã–∫—É?", "–Ø –ª—é–±–ª—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ Python.", "–î–∞–≤–∞–π —É—á–∏—Ç—å –ò–ò –≤–º–µ—Å—Ç–µ.",
        "–ú–æ–∂–µ—à—å –æ–±—ä—è—Å–Ω–∏—Ç—å –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ?", "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ?", "–°–∫–æ–ª—å–∫–æ —Ç–µ–±–µ –ª–µ—Ç?",
        "–ö–æ—Ç–æ—Ä—ã–π —Å–µ–π—á–∞—Å —á–∞—Å?", "–ü–æ–º–æ–≥–∏ —Å –¥–æ–º–∞—à–Ω–∏–º –∑–∞–¥–∞–Ω–∏–µ–º, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞.", "–° –¥–Ω—ë–º —Ä–æ–∂–¥–µ–Ω–∏—è!",
        "–ü–æ–∑–¥—Ä–∞–≤–ª—è—é —Å –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ–º!", "–£–¥–∞—á–∏!", "–í—Å–µ–≥–æ –Ω–∞–∏–ª—É—á—à–µ–≥–æ."
    ]

    bpe = BPE()
    # –£—á–∏–º BPE —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º —á–∏—Å–ª–æ–º —Å–ª–∏—è–Ω–∏–π (–¥–æ 2000, –º–∏–Ω–∏–º—É–º —á–∞—Å—Ç–æ—Ç—ã 5)
    bpe.learn_bpe(corpus, max_merges=2000, min_freq=5)

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –±–∞–∑–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –µ—Å—Ç—å
    for token in ['<unk>', '<pad>', '</w>', '<space>']:
        if token not in bpe.vocab:
            bpe.vocab[token] = len(bpe.vocab)

    model = Transformer(len(bpe.vocab), embed_size=256, nhead=8, num_layers=6, dim_feedforward=1024)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
    for param in model.parameters():
        if param.dim() > 1:
            torch.nn.init.xavier_uniform_(param)

    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss(ignore_index=bpe.vocab.get('<pad>', 0))

    sentences = [
        # English sentences
        "hello world this is a simple example of text data",
        "heaven is often described as a place of peace and happiness",
        "heavy rain can cause flooding in many areas",
        "the hell of war is something no one wants to experience",
        "hello again lets learn how to build ai models",
        "machine learning is transforming many industries",
        "natural language processing enables computers to understand human language",
        "deep neural networks have revolutionized computer vision",
        "reinforcement learning helps agents learn from interaction",
        "python is a popular programming language for AI",
        "data science combines statistics and computer science",
        "artificial intelligence is a broad field of study",
        "supervised learning requires labeled data",
        "unsupervised learning finds hidden patterns",
        "generative models can create realistic images",
        "convolutional neural networks are good for images",
        "recurrent neural networks model sequences",
        "transformers have changed the NLP landscape",
        "training large models requires significant resources",
        "optimization algorithms improve model performance",
        "regularization techniques prevent overfitting",
        "feature engineering is crucial for classical ML",
        "big data fuels modern AI applications",
        "cloud computing enables scalable training",
        "transfer learning leverages pre-trained models",
        "explainable AI helps interpret model decisions",
        "ethical AI considers fairness and bias",
        "autonomous vehicles rely on AI for navigation",
        "speech recognition converts audio to text",
        "chatbots provide automated customer support",
        "recommendation systems personalize user experience",
        "computer vision powers facial recognition",
        "AI can assist in medical diagnosis",
        "robotics integrates AI with hardware",
        "AI research advances rapidly every year",
        "deep learning models require large datasets",
        "data augmentation improves model robustness",
        "hyperparameter tuning optimizes training",
        "cross-validation assesses model generalization",
        "unsupervised clustering groups similar data",
        "reinforcement learning uses rewards for learning",
        "AI ethics addresses privacy concerns",
        "neural networks mimic brain structures",
        "AI applications include gaming and finance",
        "optimization landscapes can be complex",
        "gradient descent is a common optimizer",
        "activation functions introduce non-linearity",
        "backpropagation updates model weights",
        "batch normalization stabilizes training",
        "dropout reduces overfitting",
        "AI models can be deployed on edge devices",

        # Russian sentences
        "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        "–Ω–µ–±–æ —á–∞—Å—Ç–æ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ –º–µ—Å—Ç–æ –º–∏—Ä–∞ –∏ —Å—á–∞—Å—Ç—å—è",
        "—Å–∏–ª—å–Ω—ã–π –¥–æ–∂–¥—å –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –Ω–∞–≤–æ–¥–Ω–µ–Ω–∏—è –≤–æ –º–Ω–æ–≥–∏—Ö —Ä–∞–π–æ–Ω–∞—Ö",
        "–∞–¥ –≤–æ–π–Ω—ã ‚Äî —ç—Ç–æ —Ç–æ, —á–µ–≥–æ –Ω–∏–∫—Ç–æ –Ω–µ —Ö–æ—á–µ—Ç –∏—Å–ø—ã—Ç–∞—Ç—å",
        "–ø—Ä–∏–≤–µ—Ç —Å–Ω–æ–≤–∞ –¥–∞–≤–∞–π —É—á–∏—Ç—å—Å—è —Å–æ–∑–¥–∞–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ò–ò",
        "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –º–Ω–æ–≥–∏–µ –æ—Ç—Ä–∞—Å–ª–∏",
        "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º –ø–æ–Ω–∏–º–∞—Ç—å —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —è–∑—ã–∫",
        "–≥–ª—É–±–æ–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ",
        "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –ø–æ–º–æ–≥–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞–º —É—á–∏—Ç—å—Å—è —á–µ—Ä–µ–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ",
        "python ‚Äî –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –ò–ò",
        "–Ω–∞—É–∫–∞ –æ –¥–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫—É",
        "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç ‚Äî —à–∏—Ä–æ–∫–∞—è –æ–±–ª–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π",
        "–æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º —Ç—Ä–µ–±—É–µ—Ç —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
        "–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –Ω–∞—Ö–æ–¥–∏—Ç —Å–∫—Ä—ã—Ç—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏",
        "–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
        "—Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ —Ö–æ—Ä–æ—à–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        "—Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –º–æ–¥–µ–ª–∏—Ä—É—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
        "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏–∑–º–µ–Ω–∏–ª–∏ –ª–∞–Ω–¥—à–∞—Ñ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞",
        "–æ–±—É—á–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ç—Ä–µ–±—É–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤",
        "–∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É–ª—É—á—à–∞—é—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π",
        "—Ç–µ—Ö–Ω–∏–∫–∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ",
        "–∏–Ω–∂–µ–Ω–µ—Ä–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∞–∂–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
        "–±–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ –ø–∏—Ç–∞—é—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –ò–ò",
        "–æ–±–ª–∞—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
        "—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏",
        "–æ–±—ä—è—Å–Ω–∏–º—ã–π –ò–ò –ø–æ–º–æ–≥–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ—à–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π",
        "—ç—Ç–∏—á–µ—Å–∫–∏–π –ò–ò —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å –∏ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç—å",
        "–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–µ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –ø–æ–ª–∞–≥–∞—é—Ç—Å—è –Ω–∞ –ò–ò –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏",
        "—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç",
        "—á–∞—Ç–±–æ—Ç—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –∫–ª–∏–µ–Ω—Ç–æ–≤",
        "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç",
        "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –ª–∏—Ü",
        "–ò–ò –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –≤ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ",
        "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –ò–ò —Å –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã–º –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º",
        "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ò–ò –±—ã—Å—Ç—Ä–æ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –∫–∞–∂–¥—ã–π –≥–æ–¥",
        "–≥–ª—É–±–æ–∫–∏–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è —Ç—Ä–µ–±—É—é—Ç –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö",
        "–∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö —É–ª—É—á—à–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π",
        "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ",
        "–∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±–æ–±—â–∞–µ–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏",
        "–æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ",
        "–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è",
        "—ç—Ç–∏–∫–∞ –ò–ò –∑–∞—Ç—Ä–∞–≥–∏–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏",
        "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏–º–∏—Ç–∏—Ä—É—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –º–æ–∑–≥–∞",
        "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ò–ò –≤–∫–ª—é—á–∞—é—Ç –∏–≥—Ä—ã –∏ —Ñ–∏–Ω–∞–Ω—Å—ã",
        "–ª–∞–Ω–¥—à–∞—Ñ—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω—ã–º–∏",
        "–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ ‚Äî —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä",
        "—Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤–≤–æ–¥—è—Ç –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å",
        "–æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏",
        "–ø–∞–∫–µ—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ",
        "–¥—Ä–æ–ø–∞—É—Ç —Å–Ω–∏–∂–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ",
        "–º–æ–¥–µ–ª–∏ –ò–ò –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã –Ω–∞ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–π–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö"
    ]

    inputs, targets, padding_mask = prepare_sequences(sentences, bpe)
    if inputs is None or targets is None or padding_mask is None or inputs.size(0) == 0:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.")
        return None, None, None

    model.train()
    epochs = 50

    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(inputs, src_key_padding_mask=padding_mask)
        loss = criterion(output.view(-1, len(bpe.vocab)), targets.view(-1))

        if torch.isnan(loss) or torch.isinf(loss):
            print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω NaN/Inf –≤ loss –Ω–∞ —ç–ø–æ—Ö–µ {epoch}, –ø—Ä–µ—Ä—ã–≤–∞—é –æ–±—É—á–µ–Ω–∏–µ.")
            break

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}/{epochs}, loss: {loss.item():.4f}")

    save_model(model, optimizer, bpe)
    return model, optimizer, bpe

def save_model(model, optimizer, bpe, path='qsusli_model.pth'):
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'bpe_merges': bpe.merges,
        'bpe_vocab': bpe.vocab
    }, path)
    print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤", path)

def load_model(path='qsusli_model.pth'):
    checkpoint = torch.load(path)
    bpe = BPE()
    bpe.merges = checkpoint['bpe_merges']
    bpe.vocab = checkpoint['bpe_vocab']
    vocab_size = len(bpe.vocab)
    model = Transformer(vocab_size, embed_size=256, nhead=8, num_layers=6, dim_feedforward=1024)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer = optim.Adam(model.parameters(), lr=0.005)
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return model, optimizer, bpe

def fine_tune(model, optimizer, bpe, new_sentences, epochs=5):
    inputs, targets, padding_mask = prepare_sequences(new_sentences, bpe)
    if inputs is None or targets is None or padding_mask is None or inputs.size(0) == 0:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    criterion = nn.CrossEntropyLoss(ignore_index=bpe.vocab.get('<pad>', 0))
    model.train()

    for param_group in optimizer.param_groups:
        param_group['lr'] = 0.0001

    for epoch in range(epochs):
        optimizer.zero_grad()
        output = model(inputs, src_key_padding_mask=padding_mask)
        loss = criterion(output.view(-1, len(bpe.vocab)), targets.view(-1))

        if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 100:
            print(f"–ü—Ä–æ–±–ª–µ–º—ã —Å loss –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}: {loss.item()}, –ø—Ä–æ–ø—É—Å–∫–∞—é")
            continue

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)
        optimizer.step()

        if epoch % 2 == 0:
            print(f"Fine-tuning epoch {epoch+1}/{epochs}, loss: {loss.item():.4f}")

    save_model(model, optimizer, bpe)

def search_wikipedia(query, sentences_limit=5):
    """–ü–æ–∏—Å–∫ –≤ –í–∏–∫–∏–ø–µ–¥–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        import requests
        import re
        
        S = requests.Session()
        S.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        URL = "https://ru.wikipedia.org/w/api.php"
        
        # –û—á–∏—â–∞–µ–º –∑–∞–ø—Ä–æ—Å –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        clean_query = re.sub(r'[^\w\s]', '', query).strip()
        if not clean_query:
            print("–ü—É—Å—Ç–æ–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è Wikipedia")
            return []
        
        PARAMS = {
            "action": "query",
            "list": "search",
            "srsearch": clean_query,
            "format": "json",
            "srlimit": 2,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            "srinfo": "totalhits",
            "srprop": "snippet"
        }
        
        print(f"üîç –ü–æ–∏—Å–∫ –≤ Wikipedia: '{clean_query}'")
        
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å - –ø–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        R = S.get(url=URL, params=PARAMS, timeout=10)
        R.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        
        DATA = R.json()
        
        if 'query' not in DATA or 'search' not in DATA['query']:
            print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ Wikipedia")
            return []
        
        search_results = DATA['query']['search']
        if not search_results:
            print("–ü—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ Wikipedia")
            return []
        
        results = []
        
        for item in search_results:
            try:
                page_title = item.get('title', '')
                if not page_title:
                    continue
                
                print(f"üìñ –ü–æ–ª—É—á–∞—é —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_title}")
                
                # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å - –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                PARAMS_PAGE = {
                    "action": "query",
                    "prop": "extracts",
                    "explaintext": True,
                    "exintro": True,  # –¢–æ–ª—å–∫–æ –≤–≤–µ–¥–µ–Ω–∏–µ
                    "exlimit": 1,
                    "titles": page_title,
                    "format": "json"
                }
                
                R2 = S.get(url=URL, params=PARAMS_PAGE, timeout=10)
                R2.raise_for_status()
                
                DATA2 = R2.json()
                
                if 'query' not in DATA2 or 'pages' not in DATA2['query']:
                    continue
                
                pages = DATA2['query']['pages']
                
                for page_id in pages:
                    page_data = pages[page_id]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                    if 'missing' in page_data:
                        continue
                    
                    text = page_data.get('extract', '')
                    if not text or len(text.strip()) < 10:
                        continue
                    
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    sentences = re.split(r'[.!?]+', text)
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –æ—á–∏—â–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    for sent in sentences:
                        sent = sent.strip()
                        if len(sent) > 20 and len(sent) < 200:  # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ
                            results.append(sent)
                            if len(results) >= sentences_limit:
                                break
                    
                    if len(results) >= sentences_limit:
                        break
                        
                if len(results) >= sentences_limit:
                    break
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã '{item.get('title', '')}': {e}")
                continue
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ Wikipedia")
        return results[:sentences_limit]
        
    except requests.exceptions.Timeout:
        print("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Wikipedia")
        return []
    except requests.exceptions.RequestException as e:
        print(f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Wikipedia: {e}")
        return []
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ Wikipedia: {e}")
        return []

def analyze_semantic_meaning(text):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞"""
    try:
        import re
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        categories = {
            '–≤–æ–ø—Ä–æ—Å': ['—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—Ç–æ', '–∫–∞–∫–æ–π', '—Å–∫–æ–ª—å–∫–æ', '?'],
            '—ç–º–æ—Ü–∏—è': ['–≥—Ä—É—Å—Ç–Ω–æ', '–≤–µ—Å–µ–ª–æ', '—Ä–∞–¥–æ—Å—Ç–Ω–æ', '–ø–µ—á–∞–ª—å–Ω–æ', '–∑–ª–æ–π', '—Å—á–∞—Å—Ç–ª–∏–≤—ã–π', '—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ'],
            '–¥–µ–π—Å—Ç–≤–∏–µ': ['–¥–µ–ª–∞—Ç—å', '–∏–¥—Ç–∏', '—Ä–∞–±–æ—Ç–∞—Ç—å', '–∏–≥—Ä–∞—Ç—å', '—É—á–∏—Ç—å—Å—è', '—Å–º–æ—Ç—Ä–µ—Ç—å', '—á–∏—Ç–∞—Ç—å', '–ø–∏—Å–∞—Ç—å'],
            '–≤—Ä–µ–º—è': ['—Å–µ–≥–æ–¥–Ω—è', '–≤—á–µ—Ä–∞', '–∑–∞–≤—Ç—Ä–∞', '—Å–µ–π—á–∞—Å', '–ø–æ—Ç–æ–º', '—Å–∫–æ—Ä–æ', '–¥–∞–≤–Ω–æ', '–Ω–µ–¥–∞–≤–Ω–æ'],
            '–º–µ—Å—Ç–æ': ['–¥–æ–º', '—à–∫–æ–ª–∞', '—Ä–∞–±–æ—Ç–∞', '–≥–æ—Ä–æ–¥', '—Å—Ç—Ä–∞–Ω–∞', '–∑–¥–µ—Å—å', '—Ç–∞–º', '—Ç—É–¥–∞'],
            '–æ—Ü–µ–Ω–∫–∞': ['—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–æ—Ç–ª–∏—á–Ω–æ', '—É–∂–∞—Å–Ω–æ', '–Ω–æ—Ä–º–∞–ª—å–Ω–æ', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å'],
            '–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–ø–æ–∫–∞', '–¥–æ –≤—Å—Ç—Ä–µ—á–∏', 'hello', 'hi', 'bye'],
            '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏': ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–∫–æ–¥', '—Å–∞–π—Ç', '–∏–≥—Ä–∞', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ']
        }
        
        text_lower = text.lower()
        detected_categories = []
        
        for category, keywords in categories.items():
            if any(keyword in text_lower for keyword in keywords):
                detected_categories.append(category)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å–µ–º–∞–Ω—Ç–∏–∫—É
        if detected_categories:
            return {
                'primary_category': detected_categories[0],
                'all_categories': detected_categories,
                'confidence': len(detected_categories) / len(categories),
                'text_length': len(text.split()),
                'complexity': 'simple' if len(text.split()) <= 5 else 'complex'
            }
        
        return {
            'primary_category': 'general',
            'all_categories': ['general'],
            'confidence': 0.1,
            'text_length': len(text.split()),
            'complexity': 'simple' if len(text.split()) <= 5 else 'complex'
        }
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return None

def get_context_keywords(text):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    try:
        import re
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = ['–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '–æ', '–æ–±', '—á—Ç–æ', '—ç—Ç–æ', '–∫–∞–∫', '–Ω–æ', '–∞', '–¥–∞', '–Ω–µ—Ç', '–Ω–µ', '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–º—ã', '–≤—ã', '–æ–Ω–∏']
        
        words = re.findall(r'\b[–∞-—è—ë]+\b', text.lower())
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-5 —Å–∞–º—ã—Ö –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤
        return sorted(keywords, key=len, reverse=True)[:5]
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {e}")
        return []

def improve_unknown_word_generation(text):
    """–£–ª—É—á—à–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
    try:
        semantic_data = analyze_semantic_meaning(text)
        
        if not semantic_data:
            return None
            
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è RLHF
        return semantic_data['primary_category']
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return None

text_rlhf_policy = {}

def load_text_rlhf_policy():
    global text_rlhf_policy
    try:
        text_rlhf_policy = db.load_text_rlhf_policy()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ text RLHF –ø–æ–ª–∏—Ç–∏–∫–∏: {e}")
        text_rlhf_policy = {}

def save_text_rlhf_policy(category, data):
    try:
        db.save_text_rlhf_policy(category, data)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è text RLHF –ø–æ–ª–∏—Ç–∏–∫–∏: {e}")

def text_reinforcement_learning_update(category, reward, generation_params):
    global text_rlhf_policy
    try:
        if category not in text_rlhf_policy:
            text_rlhf_policy[category] = {
                'rewards': [],
                'avg_reward': 0.0,
                'count': 0,
                'best_params': generation_params
            }

        text_rlhf_policy[category]['rewards'].append(reward)
        text_rlhf_policy[category]['count'] += 1
        text_rlhf_policy[category]['avg_reward'] = sum(text_rlhf_policy[category]['rewards']) / len(text_rlhf_policy[category]['rewards'])

        if reward > text_rlhf_policy[category]['avg_reward']:
            text_rlhf_policy[category]['best_params'] = generation_params.copy()

        if len(text_rlhf_policy[category]['rewards']) > 100:
            text_rlhf_policy[category]['rewards'] = text_rlhf_policy[category]['rewards'][-100:]

        save_text_rlhf_policy(category, text_rlhf_policy[category])
        print(f"RLHF —Ç–µ–∫—Å—Ç: –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è '{category}', —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {text_rlhf_policy[category]['avg_reward']:.3f}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è text RLHF: {e}")

def process_rlhf_correction(query, original_response, corrected_response):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è RLHF.
    –í—ã—á–∏—Å–ª—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–æ–≤,
    –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫—É RLHF.
    """

    try:
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∏–Ω–¥–µ–∫—Å—ã
        orig_tokens = bpe.encode(original_response.lower())
        corr_tokens = bpe.encode(corrected_response.lower())

        orig_indices = torch.tensor([tokens_to_indices(orig_tokens, bpe.vocab)])
        corr_indices = torch.tensor([tokens_to_indices(corr_tokens, bpe.vocab)])

        # –ú–∞—Å–∫–∏ –ø–∞–¥–¥–∏–Ω–≥–æ–≤ (False ‚Äî –Ω–µ—Ç –ø–∞–¥–¥–∏–Ω–≥–æ–≤, —Ç.–∫. –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        orig_mask = torch.zeros_like(orig_indices, dtype=torch.bool)
        corr_mask = torch.zeros_like(corr_indices, dtype=torch.bool)

        model.eval()
        with torch.no_grad():
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è (embedding) –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º
            orig_embeds = model.embedding(orig_indices).mean(dim=1)  # (1, embed_size)
            corr_embeds = model.embedding(corr_indices).mean(dim=1)  # (1, embed_size)

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º
            similarity = F.cosine_similarity(orig_embeds, corr_embeds).item()

        # –û—Ü–µ–Ω–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è: —á–µ–º –Ω–∏–∂–µ similarity, —Ç–µ–º –±–æ–ª—å—à–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        improvement = max(0.0, 1.0 - similarity)  # –ó–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é)
        category = improve_unknown_word_generation(query) or 'default'

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã RLHF –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        current_params = get_rlhf_params_for_category(category)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, —É–º–µ–Ω—å—à–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø—Ä–∏ —É–ª—É—á—à–µ–Ω–∏–∏
        new_temperature = max(0.3, current_params.get('temperature', 0.7) - improvement * 0.1)

        new_params = current_params.copy()
        new_params['temperature'] = new_temperature

        text_reinforcement_learning_update(category, improvement, new_params)

        print(f"RLHF Correction: –∫–∞—Ç–µ–≥–æ—Ä–∏—è '{category}', —É–ª—É—á—à–µ–Ω–∏–µ {improvement:.3f}, –Ω–æ–≤–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ {new_temperature:.3f}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ RLHF –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏: {e}")

def get_rlhf_params_for_category(category):
    if category in text_rlhf_policy:
        return text_rlhf_policy[category].get('best_params', {})
    return {}

image_rlhf_policy = {}

def load_image_rlhf_policy():
    global image_rlhf_policy
    try:
        image_rlhf_policy = db.load_image_rlhf_policy()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ image RLHF –ø–æ–ª–∏—Ç–∏–∫–∏: {e}")
        image_rlhf_policy = {}

def save_image_rlhf_policy(category, data):
    try:
        db.save_image_rlhf_policy(category, data)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è image RLHF –ø–æ–ª–∏—Ç–∏–∫–∏: {e}")

def image_reinforcement_learning_update(category, reward, generation_params):
    global image_rlhf_policy
    try:
        if category not in image_rlhf_policy:
            image_rlhf_policy[category] = {
                'rewards': [],
                'avg_reward': 0.0,
                'count': 0,
                'best_params': generation_params
            }

        image_rlhf_policy[category]['rewards'].append(reward)
        image_rlhf_policy[category]['count'] += 1
        image_rlhf_policy[category]['avg_reward'] = sum(image_rlhf_policy[category]['rewards']) / len(image_rlhf_policy[category]['rewards'])

        if reward > image_rlhf_policy[category]['avg_reward']:
            image_rlhf_policy[category]['best_params'] = generation_params.copy()

        if len(image_rlhf_policy[category]['rewards']) > 100:
            image_rlhf_policy[category]['rewards'] = image_rlhf_policy[category]['rewards'][-100:]

        save_image_rlhf_policy(category, image_rlhf_policy[category])
        print(f"RLHF –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è '{category}', —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {image_rlhf_policy[category]['avg_reward']:.3f}")

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è image RLHF: {e}")

def get_image_rlhf_params_for_category(category):
    if category in image_rlhf_policy:
        return image_rlhf_policy[category].get('best_params', {})
    return {}


def download_real_images(query, num_images=3):
    if not os.path.exists('image_data'):
        os.makedirs('image_data')

    downloaded_images = []

    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä—É—Å—Å–∫–∏—Ö —Å–ª–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
    translate_dict = {
        '–∫–æ—Ç': 'cat', '–∫–æ—Ç–∏–∫': 'cat', '–∫–æ—Ç–µ–Ω–æ–∫': 'kitten', '–∫–æ—à–∫–∞': 'cat',
        '—Å–æ–±–∞–∫–∞': 'dog', '–ø–µ—Å': 'dog', '—â–µ–Ω–æ–∫': 'puppy', '—Å–æ–±–∞—á–∫–∞': 'dog',
        '–Ω–µ–±–æ': 'sky', '–æ–±–ª–∞–∫–∞': 'clouds', '—Ç—É—á–∏': 'clouds', '–æ–±–ª–∞–∫–æ': 'cloud',
        '–º–æ—Ä–µ': 'ocean', '–≤–æ–¥–∞': 'water', '–æ–∑–µ—Ä–æ': 'lake', '—Ä–µ–∫–∞': 'river',
        '–ª–µ—Å': 'forest', '–¥–µ—Ä–µ–≤–æ': 'tree', '–¥–µ—Ä–µ–≤—å—è': 'trees', '–ø–∞—Ä–∫': 'park',
        '–≥–æ—Ä–∞': 'mountain', '–≥–æ—Ä—ã': 'mountains', '—Ö–æ–ª–º': 'hill',
        '—Ü–≤–µ—Ç–æ–∫': 'flower', '—Ü–≤–µ—Ç—ã': 'flowers', '—Ä–æ–∑–∞': 'rose',
        '—Å–æ–ª–Ω—Ü–µ': 'sun', '–ª—É–Ω–∞': 'moon', '–∑–≤–µ–∑–¥—ã': 'stars', '–Ω–µ–±–µ—Å–∞': 'sky',
        '–¥–æ–º': 'house', '–∑–¥–∞–Ω–∏–µ': 'building', '–∫–≤–∞—Ä—Ç–∏—Ä–∞': 'apartment',
        '–º–∞—à–∏–Ω–∞': 'car', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å': 'car', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç': 'transport',
        '–ø—Ç–∏—Ü–∞': 'bird', '—Ä—ã–±–∞': 'fish', '–∂–∏–≤–æ—Ç–Ω–æ–µ': 'animal',
        '–æ–≥–æ–Ω—å': 'fire', '–ø–ª–∞–º—è': 'flame', '–∂–∞—Ä': 'heat',
        '—Å–Ω–µ–≥': 'snow', '–∑–∏–º–∞': 'winter', '–º–æ—Ä–æ–∑': 'frost',
        '–ª–µ—Ç–æ': 'summer', '–≤–µ—Å–Ω–∞': 'spring', '–æ—Å–µ–Ω—å': 'autumn',
        '—Ä–∞–¥–æ—Å—Ç—å': 'joy', '–ø–µ—á–∞–ª—å': 'sadness', '–≥—Ä—É—Å—Ç—å': 'sadness',
        '–≥–æ—Ä–æ–¥': 'city', '–º–æ—Å—Ç': 'bridge', '—É–ª–∏—Ü–∞': 'street',
        '–ø–ª—è–∂': 'beach', '–∫–æ—Ä–∞–±–ª—å': 'ship', '–ª–æ–¥–∫–∞': 'boat',
        '—Ç—É–º–∞–Ω': 'fog', '–¥—ã–º': 'smoke', '–±—É—Ä—è': 'storm',
        '–≤–µ—Ç–µ—Ä': 'wind', '–¥–æ–∂–¥—å': 'rain', '–±—É—Ä—è': 'storm',
        '–∑–≤—É–∫': 'sound', '–º—É–∑—ã–∫–∞': 'music', '–ø–µ—Å–Ω—è': 'song',
        '–µ–¥–∞': 'food', '—Ñ—Ä—É–∫—Ç—ã': 'fruits', '–æ–≤–æ—â–∏': 'vegetables',
        '–∫–æ—Ñ–µ': 'coffee', '—á–∞–π': 'tea', '–≤–∏–Ω–æ': 'wine',
        '—Å–ø–æ—Ä—Ç': 'sport', '—Ñ—É—Ç–±–æ–ª': 'football', '–±–∞—Å–∫–µ—Ç–±–æ–ª': 'basketball',
        '–∏–≥—Ä–∞': 'game', '–∫–æ–º–ø—å—é—Ç–µ—Ä': 'computer', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç': 'internet',
        '–ø—Ä–æ–≥—Ä–∞–º–º–∞': 'program', '–∫–æ–¥': 'code', '—Å–∞–π—Ç': 'website',
        '—Ç–µ–ª–µ—Ñ–æ–Ω': 'phone', '—Ç–µ–ª–µ–≤–∏–∑–æ—Ä': 'tv', '—Ä–∞–¥–∏–æ': 'radio',
        '—É—á—ë–±–∞': 'study', '—à–∫–æ–ª–∞': 'school', '—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç': 'university',
        '—Ä–∞–±–æ—Ç–∞': 'work', '–æ—Ñ–∏—Å': 'office', '–ø—Ä–∞–∑–¥–Ω–∏–∫': 'holiday',
        '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ': 'travel', '–æ—Ç–ø—É—Å–∫': 'vacation', '–æ—Ç–¥—ã—Ö': 'rest',
        '–ª—é–±–æ–≤—å': 'love', '–¥—Ä—É–∂–±–∞': 'friendship', '—Å–µ–º—å—è': 'family',
        '—Å–æ–ª–Ω—Ü–µ —Å–≤–µ—Ç–∏—Ç': 'sun shines', '—Å–≤–µ—Ç–∏—Ç –ª—É–Ω–∞': 'moon shines',
        '–≥–æ—Ä–∏—Ç –æ–≥–æ–Ω—å': 'fire burns', '–ø–∞–¥–∞–µ—Ç —Å–Ω–µ–≥': 'snow falls',
        '–ª–µ—Ç–∏—Ç –ø—Ç–∏—Ü–∞': 'bird flies', '–ø–ª—ã–≤–µ—Ç –∫–æ—Ä–∞–±–ª—å': 'ship sails',
        '–±–µ–∂–∏—Ç —Å–æ–±–∞–∫–∞': 'dog runs', '—Ä–∞—Å—Ç–µ—Ç –¥–µ—Ä–µ–≤–æ': 'tree grows'
    }

    words = query.lower().split()
    english_words = []
    for word in words:
        if word in translate_dict:
            english_words.append(translate_dict[word])
        else:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ
            found = False
            for ru_word, en_word in translate_dict.items():
                if word in ru_word or ru_word in word:
                    english_words.append(en_word)
                    found = True
                    break
            if not found:
                english_words.append(word)

    search_query = ' '.join(english_words) if english_words else query

    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    image_sources = [
        lambda q, i: f"https://source.unsplash.com/800x600/?{urllib.parse.quote(q.replace(' ', ','))}",
        lambda q, i: f"https://picsum.photos/800/600?random={i}&blur=2",
        lambda q, i: f"https://source.unsplash.com/featured/800x600/?{urllib.parse.quote(q.replace(' ', ','))},nature",
        lambda q, i: f"https://source.unsplash.com/collection/190727/800x600/?{urllib.parse.quote(q.replace(' ', ','))}"
    ]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    print(f"üîç –ù–∞—á–∏–Ω–∞—é –∑–∞–≥—Ä—É–∑–∫—É {num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{search_query}'")

    for i in range(num_images):
        downloaded = False
        attempts = 0
        max_attempts = 3

        while not downloaded and attempts < max_attempts:
            try:
                attempts += 1
                source_idx = (i + attempts - 1) % len(image_sources)
                url = image_sources[source_idx](search_query, i + attempts)

                print(f"üì• –ü–æ–ø—ã—Ç–∫–∞ {attempts}/{max_attempts} –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}")
                print(f"üåê –ó–∞–≥—Ä—É–∂–∞—é –∏–∑: {url}")

                response = requests.get(url, headers=headers, timeout=30)

                if response.status_code == 200 and len(response.content) > 1000:
                    file_hash = hashlib.md5(f"{query}_{search_query}_{i}_{attempts}".encode('utf-8')).hexdigest()
                    filename = f"image_data/real_{file_hash}.jpg"

                    with open(filename, 'wb') as f:
                        f.write(response.content)

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    try:
                        with Image.open(filename) as img:
                            img.verify()
                        downloaded_images.append(filename)
                        downloaded = True
                        print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}/{num_images} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {filename}")
                        time.sleep(0.5)
                    except Exception as e:
                        print(f"‚ùå –°–∫–∞—á–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {e}")
                        if os.path.exists(filename):
                            os.remove(filename)
                else:
                    print(f"‚ö†Ô∏è –ü–ª–æ—Ö–æ–π –æ—Ç–≤–µ—Ç: status={response.status_code}, size={len(response.content)}")

            except requests.exceptions.Timeout:
                print(f"‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}, –ø–æ–ø—ã—Ç–∫–∞ {attempts}")
                time.sleep(1)

            except requests.exceptions.RequestException as e:
                print(f"üåê –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}: {e}")
                time.sleep(1)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}: {e}")
                time.sleep(1)

        if not downloaded:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1} –ø–æ—Å–ª–µ {max_attempts} –ø–æ–ø—ã—Ç–æ–∫")

    print(f"üìä –ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(downloaded_images)}/{num_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    return downloaded_images

def load_improved_params():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.
    """
    try:
        rows = db.select_from('improved_generation_params')
        params = {}
        for row in rows:
            params[row['category']] = json.loads(row['best_params'])
        return params
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ –ë–î: {e}")
        return {}

def generate_smart_base_images(query, num_images=3):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    if not os.path.exists('image_data'):
        os.makedirs('image_data')

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    improved_params = load_improved_params()

    # –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ç–µ–º—ã —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    themes = {
        # –ü—Ä–∏—Ä–æ–¥–∞
        '–Ω–µ–±–æ': {'colors': [(87, 160, 211), (130, 200, 240), (255, 255, 255), (220, 240, 255)], 'type': 'sky'},
        'sky': {'colors': [(87, 160, 211), (130, 200, 240), (255, 255, 255), (220, 240, 255)], 'type': 'sky'},
        '–º–æ—Ä–µ': {'colors': [(32, 78, 136), (64, 164, 223), (135, 206, 235), (0, 139, 204)], 'type': 'ocean'},
        'ocean': {'colors': [(32, 78, 136), (64, 164, 223), (135, 206, 235), (0, 139, 204)], 'type': 'ocean'},
        '–ª–µ—Å': {'colors': [(34, 80, 34), (60, 120, 60), (85, 140, 85), (40, 60, 40)], 'type': 'forest'},
        'forest': {'colors': [(34, 80, 34), (60, 120, 60), (85, 140, 85), (40, 60, 40)], 'type': 'forest'},
        '—Å–æ–ª–Ω—Ü–µ': {'colors': [(255, 200, 0), (255, 165, 0), (255, 140, 0), (255, 215, 0)], 'type': 'sun'},
        'sun': {'colors': [(255, 200, 0), (255, 165, 0), (255, 140, 0), (255, 215, 0)], 'type': 'sun'},

        # –ñ–∏–≤–æ—Ç–Ω—ã–µ
        '–∫–æ—Ç': {'colors': [(160, 110, 70), (200, 150, 100), (180, 130, 85), (140, 90, 50)], 'type': 'cat'},
        'cat': {'colors': [(160, 110, 70), (200, 150, 100), (180, 130, 85), (140, 90, 50)], 'type': 'cat'},
        '—Å–æ–±–∞–∫–∞': {'colors': [(120, 80, 50), (160, 110, 70), (200, 150, 100), (100, 60, 30)], 'type': 'dog'},
        'dog': {'colors': [(120, 80, 50), (160, 110, 70), (200, 150, 100), (100, 60, 30)], 'type': 'dog'},

        # –û–≥–æ–Ω—å
        '–æ–≥–æ–Ω—å': {'colors': [(200, 40, 0), (255, 100, 0), (255, 150, 0), (255, 200, 50)], 'type': 'fire'},
        'fire': {'colors': [(200, 40, 0), (255, 100, 0), (255, 150, 0), (255, 200, 50)], 'type': 'fire'},

        # –ö–æ—Å–º–æ—Å
        'space': {'colors': [(10, 10, 40), (25, 25, 80), (50, 50, 120), (5, 5, 20)], 'type': 'space'},
        '–∫–æ—Å–º–æ—Å': {'colors': [(10, 10, 40), (25, 25, 80), (50, 50, 120), (5, 5, 20)], 'type': 'space'},

        # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        'default': {'colors': [(100, 150, 200), (150, 100, 200), (200, 100, 150), (120, 180, 160)], 'type': 'abstract'}
    }

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É
    query_lower = query.lower()
    theme = None
    for key in themes:
        if key in query_lower:
            theme = themes[key]
            break

    if not theme:
        # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ
        try:
            if 'improve_unknown_word_generation' in locals():
                suggested_theme = improve_unknown_word_generation(query)
                if suggested_theme:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥—Ö–æ–¥—è—â—É—é —Ç–µ–º—É –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
                    theme = themes.get(suggested_theme, themes['default'])
                    print(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—è '{suggested_theme}' –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}'")

                    # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                    if suggested_theme in improved_params:
                        improved_data = improved_params[suggested_theme]
                        print(f"–ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è '{suggested_theme}' (—Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {improved_data.get('avg_score', 0):.3f})")

                        # –ú–æ–∂–µ–º –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å —Ü–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
                        if improved_data.get('avg_score', 0) > 0.7:
                            # –î–ª—è –æ—á–µ–Ω—å —É—Å–ø–µ—à–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —è—Ä–∫–∏–µ —Ü–≤–µ—Ç–∞
                            if 'colors' in theme:
                                theme['colors'] = [
                                    tuple(min(255, int(c * 1.2)) for c in color) 
                                    for color in theme['colors']
                                ]
                else:
                    theme = themes['default']
            else:
                theme = themes['default']
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ç–µ–º—ã: {e}")
            theme = themes['default']

    generated_images = []

    for i in range(num_images):
        try:
            # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            img = Image.new('RGB', (800, 800))
            draw = ImageDraw.Draw(img)

            # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Ñ–æ–Ω
            def create_gradient_background(img, colors):
                """–°–æ–∑–¥–∞–µ—Ç –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Ñ–æ–Ω"""
                width, height = img.size
                
                # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞–¥–∏–µ–Ω—Ç
                for y in range(height):
                    color_progress = y / height
                    
                    # –í—ã–±–∏—Ä–∞–µ–º –º–µ–∂–¥—É —Ü–≤–µ—Ç–∞–º–∏ –ø–ª–∞–≤–Ω–æ
                    color_index = color_progress * (len(colors) - 1)
                    base_idx = int(color_index)
                    next_idx = min(base_idx + 1, len(colors) - 1)
                    blend_factor = color_index - base_idx
                    
                    base_color = colors[base_idx]
                    next_color = colors[next_idx]
                    
                    r = int(base_color[0] * (1 - blend_factor) + next_color[0] * blend_factor)
                    g = int(base_color[1] * (1 - blend_factor) + next_color[1] * blend_factor)
                    b = int(base_color[2] * (1 - blend_factor) + next_color[2] * blend_factor)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ —Å–ª—É—á–∞–π–Ω–æ–µ –≤–∞—Ä—å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
                    r = max(0, min(255, r + random.randint(-5, 5)))
                    g = max(0, min(255, g + random.randint(-5, 5)))
                    b = max(0, min(255, b + random.randint(-5, 5)))
                    
                    draw.line([(0, y), (width, y)], fill=(r, g, b))

            create_gradient_background(img, theme['colors'])

            # –†–∏—Å—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞
            img_type = theme.get('type', 'abstract')

            def draw_realistic_clouds(draw, colors, width, height):
                """–†–∏—Å—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –æ–±–ª–∞–∫–∞"""
                for _ in range(random.randint(5, 12)):
                    # –û—Å–Ω–æ–≤–Ω–æ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –æ–±–ª–∞–∫–∞
                    cx = random.randint(0, width)
                    cy = random.randint(0, height // 2)
                    
                    # –°–æ–∑–¥–∞–µ–º –æ–±–ª–∞–∫–æ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏—Ö—Å—è –∫—Ä—É–≥–æ–≤
                    num_circles = random.randint(8, 15)
                    base_size = random.randint(40, 80)
                    
                    cloud_color = (255, 255, 255, random.randint(180, 220))
                    
                    for _ in range(num_circles):
                        offset_x = random.randint(-base_size, base_size)
                        offset_y = random.randint(-base_size//2, base_size//2)
                        circle_size = base_size + random.randint(-20, 20)
                        
                        x = cx + offset_x
                        y = cy + offset_y
                        
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ —Ü–≤–µ—Ç–∞
                        alpha = random.randint(150, 200)
                        soft_color = (255, 255, 255)
                        
                        draw.ellipse([x, y, x + circle_size, y + circle_size//2], fill=soft_color)

            def draw_realistic_ocean(draw, colors, width, height):
                """–†–∏—Å—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤–æ–ª–Ω—ã"""
                # –†–∏—Å—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤ –≤–æ–ª–Ω
                for layer in range(6):
                    y_base = height // 2 + layer * 60
                    wave_color = colors[layer % len(colors)]
                    
                    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—É—Å–æ–∏–¥–∞–ª—å–Ω—ã–µ –≤–æ–ª–Ω—ã
                    points = []
                    for x in range(0, width + 20, 8):
                        wave1 = 15 * math.sin(x * 0.02 + layer * 1.5)
                        wave2 = 8 * math.sin(x * 0.05 + layer * 2)
                        wave3 = 4 * math.sin(x * 0.1 + layer * 3)
                        
                        y = y_base + wave1 + wave2 + wave3
                        points.extend([x, y])
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∏–∂–Ω–∏–π –∫—Ä–∞–π –¥–ª—è –∑–∞–ª–∏–≤–∫–∏
                    points.extend([width, height, 0, height])
                    
                    if len(points) >= 6:
                        draw.polygon(points, fill=wave_color)

            def draw_realistic_forest(draw, colors, width, height):
                """–†–∏—Å—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –ª–µ—Å"""
                # –†–∏—Å—É–µ–º –¥–µ—Ä–µ–≤—å—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤
                for _ in range(random.randint(8, 15)):
                    x = random.randint(0, width)
                    tree_height = random.randint(100, 200)
                    tree_width = random.randint(20, 40)
                    
                    # –°—Ç–≤–æ–ª
                    trunk_color = (101, 67, 33)
                    trunk_width = tree_width // 4
                    draw.rectangle([
                        x - trunk_width//2, height - 30,
                        x + trunk_width//2, height - tree_height//3
                    ], fill=trunk_color)
                    
                    # –ö—Ä–æ–Ω–∞ - –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–µ–≤ –∑–µ–ª–µ–Ω–æ–≥–æ
                    crown_layers = 3
                    for layer in range(crown_layers):
                        crown_size = tree_width - layer * 5
                        crown_y = height - tree_height//3 - layer * 20
                        crown_color = colors[layer % len(colors)]
                        
                        # –†–∏—Å—É–µ–º –∫—Ä–æ–Ω—É –∫–∞–∫ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫—Ä—É–≥
                        for _ in range(6):
                            offset_x = random.randint(-crown_size//3, crown_size//3)
                            offset_y = random.randint(-crown_size//4, crown_size//4)
                            circle_size = crown_size + random.randint(-5, 5)
                            
                            draw.ellipse([
                                x + offset_x - circle_size//2,
                                crown_y + offset_y - circle_size//2,
                                x + offset_x + circle_size//2,
                                crown_y + offset_y + circle_size//2
                            ], fill=crown_color)

            def draw_realistic_fire(draw, colors, width, height):
                """–†–∏—Å—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –ø–ª–∞–º—è"""
                # –û—Å–Ω–æ–≤–∞–Ω–∏–µ –æ–≥–Ω—è
                base_y = height - 50
                base_width = width // 3
                base_x = width // 2 - base_width // 2
                
                # –†–∏—Å—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ —è–∑—ã–∫–æ–≤ –ø–ª–∞–º–µ–Ω–∏
                for flame in range(12):
                    flame_x = base_x + random.randint(0, base_width)
                    flame_height = random.randint(80, 180)
                    flame_width = random.randint(15, 35)
                    
                    # –°–æ–∑–¥–∞–µ–º –∏–∑–≥–∏–±–∞—é—â–∏–π—Å—è —è–∑—ã–∫ –ø–ª–∞–º–µ–Ω–∏
                    points = []
                    segments = 8
                    
                    for seg in range(segments + 1):
                        y_pos = base_y - (seg / segments) * flame_height
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≥–∏–±
                        curve_offset = 15 * math.sin(seg * 0.8) * (seg / segments)
                        x_pos = flame_x + curve_offset
                        
                        # –®–∏—Ä–∏–Ω–∞ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –∫ –≤–µ—Ä—Ö—É
                        width_factor = 1 - (seg / segments) * 0.7
                        current_width = flame_width * width_factor
                        
                        # –õ–µ–≤–∞—è –∏ –ø—Ä–∞–≤–∞—è —Å—Ç–æ—Ä–æ–Ω—ã –ø–ª–∞–º–µ–Ω–∏
                        if seg == 0:
                            points.extend([x_pos - current_width//2, y_pos])
                        points.extend([x_pos + current_width//2, y_pos])
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤—É—é —Å—Ç–æ—Ä–æ–Ω—É –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                    for seg in range(segments, -1, -1):
                        y_pos = base_y - (seg / segments) * flame_height
                        curve_offset = 15 * math.sin(seg * 0.8) * (seg / segments)
                        x_pos = flame_x + curve_offset
                        width_factor = 1 - (seg / segments) * 0.7
                        current_width = flame_width * width_factor
                        
                        points.extend([x_pos - current_width//2, y_pos])
                    
                    if len(points) >= 6:
                        flame_color = colors[flame % len(colors)]
                        draw.polygon(points, fill=flame_color)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ —Ä–∏—Å–æ–≤–∞–Ω–∏—è
            if img_type == 'sky':
                draw_realistic_clouds(draw, theme['colors'], img.width, img.height)
            elif img_type == 'ocean':
                draw_realistic_ocean(draw, theme['colors'], img.width, img.height)
            elif img_type == 'forest':
                draw_realistic_forest(draw, theme['colors'], img.width, img.height)
            elif img_type == 'fire':
                draw_realistic_fire(draw, theme['colors'], img.width, img.height)

            elif img_type == 'cat':
                def draw_realistic_cat(draw, colors, width, height):
                    """–†–∏—Å—É–µ—Ç –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ –∫–æ—Ç–∞"""
                    # –ü–æ–∑–∏—Ü–∏—è –∫–æ—Ç–∞
                    cat_x = width // 2 - 100
                    cat_y = height // 2 - 50
                    
                    main_color = colors[0]
                    accent_color = colors[1] if len(colors) > 1 else main_color
                    
                    # –¢–µ–ª–æ –∫–æ—Ç–∞ (–æ–≤–∞–ª—å–Ω–æ–µ)
                    body_width, body_height = 120, 80
                    draw.ellipse([
                        cat_x, cat_y + 40,
                        cat_x + body_width, cat_y + 40 + body_height
                    ], fill=main_color)
                    
                    # –ì–æ–ª–æ–≤–∞ (–∫—Ä—É–≥–ª–∞—è)
                    head_size = 70
                    head_x = cat_x + 25
                    head_y = cat_y
                    draw.ellipse([
                        head_x, head_y,
                        head_x + head_size, head_y + head_size
                    ], fill=main_color)
                    
                    # –£—à–∏ (—Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∏)
                    ear_size = 25
                    # –õ–µ–≤–æ–µ —É—Ö–æ
                    draw.polygon([
                        (head_x + 10, head_y + 15),
                        (head_x + 10 + ear_size, head_y),
                        (head_x + 20 + ear_size, head_y + 20)
                    ], fill=main_color)
                    # –ü—Ä–∞–≤–æ–µ —É—Ö–æ
                    draw.polygon([
                        (head_x + 30, head_y + 15),
                        (head_x + 30 + ear_size, head_y),
                        (head_x + 40 + ear_size, head_y + 20)
                    ], fill=main_color)
                    
                    # –ì–ª–∞–∑–∞ (–∑–µ–ª–µ–Ω—ã–µ –æ–≤–∞–ª—ã)
                    eye_color = (34, 139, 34)
                    draw.ellipse([head_x + 15, head_y + 25, head_x + 25, head_y + 35], fill=eye_color)
                    draw.ellipse([head_x + 40, head_y + 25, head_x + 50, head_y + 35], fill=eye_color)
                    
                    # –ó—Ä–∞—á–∫–∏
                    draw.ellipse([head_x + 18, head_y + 28, head_x + 22, head_y + 32], fill=(0, 0, 0))
                    draw.ellipse([head_x + 43, head_y + 28, head_x + 47, head_y + 32], fill=(0, 0, 0))
                    
                    # –ù–æ—Å (—Ä–æ–∑–æ–≤—ã–π —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫)
                    nose_color = (255, 182, 193)
                    draw.polygon([
                        (head_x + 32, head_y + 40),
                        (head_x + 38, head_y + 40),
                        (head_x + 35, head_y + 45)
                    ], fill=nose_color)
                    
                    # –£—Å—ã
                    whisker_color = (64, 64, 64)
                    for i in range(3):
                        # –õ–µ–≤—ã–µ —É—Å—ã
                        y_pos = head_y + 45 + i * 3
                        draw.line([head_x - 10, y_pos, head_x + 20, y_pos], fill=whisker_color, width=1)
                        # –ü—Ä–∞–≤—ã–µ —É—Å—ã
                        draw.line([head_x + 50, y_pos, head_x + 80, y_pos], fill=whisker_color, width=1)
                    
                    # –•–≤–æ—Å—Ç (–∏–∑–æ–≥–Ω—É—Ç—ã–π)
                    tail_points = []
                    tail_start_x = cat_x + body_width - 10
                    tail_start_y = cat_y + 60
                    
                    for i in range(15):
                        angle = i * 0.3
                        x = tail_start_x + i * 8 + 20 * math.sin(angle)
                        y = tail_start_y - i * 4 + 10 * math.cos(angle)
                        tail_points.extend([x, y])
                    
                    if len(tail_points) >= 4:
                        for i in range(0, len(tail_points) - 2, 4):
                            if i + 3 < len(tail_points):
                                draw.line([tail_points[i], tail_points[i+1], tail_points[i+2], tail_points[i+3]], 
                                         fill=accent_color, width=8)

                draw_realistic_cat(draw, theme['colors'], img.width, img.height)

            elif img_type == 'sun':
                def draw_realistic_sun(draw, colors, width, height):
                    """–†–∏—Å—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ —Å–æ–ª–Ω—Ü–µ"""
                    sun_x = width // 2
                    sun_y = height // 3
                    sun_radius = 80
                    
                    # –û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–ª–Ω—Ü–µ (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –∫—Ä—É–≥)
                    for r in range(sun_radius, 0, -3):
                        intensity = 1 - (r / sun_radius)
                        color_idx = int(intensity * (len(colors) - 1))
                        color = colors[min(color_idx, len(colors) - 1)]
                        
                        draw.ellipse([
                            sun_x - r, sun_y - r,
                            sun_x + r, sun_y + r
                        ], fill=color)
                    
                    # –õ—É—á–∏ —Å–æ–ª–Ω—Ü–∞
                    ray_color = colors[-1]
                    for angle in range(0, 360, 30):
                        rad = math.radians(angle)
                        start_x = sun_x + (sun_radius + 10) * math.cos(rad)
                        start_y = sun_y + (sun_radius + 10) * math.sin(rad)
                        end_x = sun_x + (sun_radius + 40) * math.cos(rad)
                        end_y = sun_y + (sun_radius + 40) * math.sin(rad)
                        
                        draw.line([start_x, start_y, end_x, end_y], fill=ray_color, width=4)

                draw_realistic_sun(draw, theme['colors'], img.width, img.height)

            elif img_type == 'space':
                def draw_realistic_space(draw, colors, width, height):
                    """–†–∏—Å—É–µ—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –∫–æ—Å–º–æ—Å"""
                    # –ó–≤–µ–∑–¥—ã —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —è—Ä–∫–æ—Å—Ç–∏
                    for _ in range(150):
                        x = random.randint(0, width)
                        y = random.randint(0, height)
                        
                        if random.random() > 0.95:  # –Ø—Ä–∫–∏–µ –∑–≤–µ–∑–¥—ã
                            size = random.randint(3, 6)
                            brightness = random.randint(200, 255)
                            star_color = (brightness, brightness, brightness)
                            
                            # –ö—Ä–µ—Å—Ç–æ–æ–±—Ä–∞–∑–Ω–∞—è –∑–≤–µ–∑–¥–∞
                            draw.line([x-size, y, x+size, y], fill=star_color, width=1)
                            draw.line([x, y-size, x, y+size], fill=star_color, width=1)
                            draw.ellipse([x-1, y-1, x+1, y+1], fill=star_color)
                        else:  # –û–±—ã—á–Ω—ã–µ –∑–≤–µ–∑–¥—ã
                            size = random.randint(1, 3)
                            brightness = random.randint(150, 220)
                            star_color = (brightness, brightness, brightness)
                            draw.ellipse([x, y, x+size, y+size], fill=star_color)
                    
                    # –¢—É–º–∞–Ω–Ω–æ—Å—Ç–∏
                    for _ in range(3):
                        nebula_x = random.randint(100, width-100)
                        nebula_y = random.randint(100, height-100)
                        nebula_size = random.randint(60, 120)
                        
                        nebula_color = colors[random.randint(1, len(colors)-1)]
                        
                        # –°–æ–∑–¥–∞–µ–º —Ç—É–º–∞–Ω–Ω–æ—Å—Ç—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–∑—Ä–∞—á–Ω—ã—Ö –∫—Ä—É–≥–æ–≤
                        for _ in range(8):
                            offset_x = random.randint(-nebula_size//2, nebula_size//2)
                            offset_y = random.randint(-nebula_size//2, nebula_size//2)
                            circle_size = random.randint(nebula_size//3, nebula_size)
                            
                            # –î–µ–ª–∞–µ–º —Ü–≤–µ—Ç –±–æ–ª–µ–µ –ø—Ä–æ–∑—Ä–∞—á–Ω—ã–º
                            faded_color = tuple(int(c * 0.3) for c in nebula_color)
                            
                            draw.ellipse([
                                nebula_x + offset_x - circle_size//2,
                                nebula_y + offset_y - circle_size//2,
                                nebula_x + offset_x + circle_size//2,
                                nebula_y + offset_y + circle_size//2
                            ], fill=faded_color)

                draw_realistic_space(draw, theme['colors'], img.width, img.height)

            else:  # abstract –∏ –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã
                def draw_realistic_abstract(draw, colors, width, height):
                    """–†–∏—Å—É–µ—Ç –∫—Ä–∞—Å–∏–≤—É—é –∞–±—Å—Ç—Ä–∞–∫—Ü–∏—é"""
                    # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–≤–Ω—ã–µ –≤–æ–ª–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã
                    for layer in range(5):
                        points = []
                        layer_color = colors[layer % len(colors)]
                        
                        # –°–æ–∑–¥–∞–µ–º –ø–ª–∞–≤–Ω—É—é –∫—Ä–∏–≤—É—é
                        for x in range(0, width + 50, 20):
                            wave1 = 30 * math.sin(x * 0.01 + layer)
                            wave2 = 20 * math.sin(x * 0.02 + layer * 2)
                            wave3 = 10 * math.sin(x * 0.05 + layer * 3)
                            
                            y = height // 2 + wave1 + wave2 + wave3 + layer * 40
                            points.extend([x, y])
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—è –¥–ª—è –∑–∞–ª–∏–≤–∫–∏
                        points.extend([width, height, 0, height])
                        
                        if len(points) >= 6:
                            draw.polygon(points, fill=layer_color)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    for _ in range(8):
                        x = random.randint(50, width - 50)
                        y = random.randint(50, height - 50)
                        size = random.randint(20, 60)
                        color = colors[random.randint(0, len(colors) - 1)]
                        
                        if random.random() > 0.5:
                            # –ö—Ä—É–≥–∏ —Å –º—è–≥–∫–∏–º–∏ –∫—Ä–∞—è–º–∏
                            for r in range(size, 0, -5):
                                alpha = 1 - (r / size)
                                soft_color = tuple(int(c * alpha + 255 * (1 - alpha)) for c in color)
                                draw.ellipse([x - r//2, y - r//2, x + r//2, y + r//2], fill=soft_color)
                        else:
                            # –ú–Ω–æ–≥–æ—É–≥–æ–ª—å–Ω–∏–∫–∏
                            sides = random.randint(3, 8)
                            poly_points = []
                            for angle in range(0, 360, 360 // sides):
                                rad = math.radians(angle)
                                px = x + size * math.cos(rad)
                                py = y + size * math.sin(rad)
                                poly_points.extend([px, py])
                            
                            if len(poly_points) >= 6:
                                draw.polygon(poly_points, fill=color)

                draw_realistic_abstract(draw, theme['colors'], img.width, img.height)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            enhancer = ImageEnhance.Contrast(img)
            img = enhancer.enhance(1.2)
            
            # –ù–µ–º–Ω–æ–≥–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç—å
            enhancer = ImageEnhance.Color(img)
            img = enhancer.enhance(1.1)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º—è–≥–∫–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
            img = img.filter(ImageFilter.SMOOTH_MORE)
            
            # –£–º–µ–Ω—å—à–∞–µ–º –¥–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Å –∞–Ω—Ç–∏–∞–ª–∏–∞—Å–∏–Ω–≥–æ–º
            img = img.resize((512, 512), Image.Resampling.LANCZOS)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            filename = f"image_data/smart_{hashlib.md5(f'{query}_{i}_{time.time()}'.encode('utf-8')).hexdigest()}.jpg"
            img.save(filename, quality=90)
            generated_images.append(filename)
            
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}/{num_images}: {filename}")

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}: {e}")

    return generated_images

def analyze_image_colors(image_path):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ü–≤–µ—Ç–æ–≤—É—é –ø–∞–ª–∏—Ç—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        with Image.open(image_path) as img:
            # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            img = img.resize((100, 100))
            img = img.convert('RGB')

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∏–∫—Å–µ–ª–∏
            pixels = list(img.getdata())

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–µ —Ü–≤–µ—Ç–∞
            color_counts = {}
            for pixel in pixels:
                color_counts[pixel] = color_counts.get(pixel, 0) + 1

            # –¢–æ–ø-5 —Ü–≤–µ—Ç–æ–≤
            top_colors = sorted(color_counts.items(), key=lambda x: x[1], reverse=True)[:5]

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —è—Ä–∫–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            avg_brightness = sum(sum(pixel) for pixel in pixels) / (len(pixels) * 3)

            return {
                'dominant_colors': [color[0] for color in top_colors],
                'brightness': avg_brightness,
                'contrast': max(pixels)[0] - min(pixels)[0] if pixels else 0
            }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
        return None

def generate_from_dataset(downloaded_images, query):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    if not downloaded_images:
        return None

    try:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å–∫–∞—á–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        color_palette = []
        brightness_values = []

        for img_path in downloaded_images:
            analysis = analyze_image_colors(img_path)
            if analysis:
                color_palette.extend(analysis['dominant_colors'])
                brightness_values.append(analysis['brightness'])

        if not color_palette:
            return None

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        img = Image.new('RGB', (512, 512))
        draw = ImageDraw.Draw(img)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–≤–µ—Ç–∞ –¥–ª—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
        avg_brightness = sum(brightness_values) / len(brightness_values) if brightness_values else 128

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –∏–∑ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏—Ö —Ü–≤–µ—Ç–æ–≤
        for y in range(512):
            color_index = int((y / 512) * (len(color_palette) - 1))
            if color_index < len(color_palette):
                color = color_palette[color_index]

                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —è—Ä–∫–æ—Å—Ç—å
                adjusted_color = tuple(
                    min(255, max(0, int(c * (avg_brightness / 128))))
                    for c in color
                )

                draw.line([(0, y), (512, y)], fill=adjusted_color)

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Ñ–æ—Ä–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        for _ in range(random.randint(3, 8)):
            x = random.randint(0, 400)
            y = random.randint(0, 400)
            size = random.randint(30, 100)

            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ü–≤–µ—Ç –∏–∑ –ø–∞–ª–∏—Ç—Ä—ã
            color = random.choice(color_palette)

            # –†–∏—Å—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º—ã
            shape_type = random.choice(['circle', 'rectangle', 'triangle'])

            if shape_type == 'circle':
                draw.ellipse([x, y, x+size, y+size], fill=color)
            elif shape_type == 'rectangle':
                draw.rectangle([x, y, x+size, y+size], fill=color)
            elif shape_type == 'triangle':
                draw.polygon([
                    (x, y+size),
                    (x+size//2, y),
                    (x+size, y+size)
                ], fill=color)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if random.random() > 0.5:
            img = img.filter(ImageFilter.SMOOTH)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        filename = f"image_data/dataset_generated_{hashlib.md5(f'{query}_{time.time()}'.encode('utf-8')).hexdigest()}.jpg"
        img.save(filename)

        return filename

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        return None

def intelligent_image_generation(prompt, bpe, max_images=5):
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –≤–∞—à–µ–π –∏–¥–µ–µ"""
    print(f"üß† –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è: '{prompt}'")

    # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
    tokens = bpe.encode(prompt.lower())
    words = prompt.lower().split()

    print(f"üìù –¢–æ–∫–µ–Ω—ã: {tokens}")
    print(f"üî§ –°–ª–æ–≤–∞: {words}")

    # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    search_queries = []

    # –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—Ä–æ—Å
    search_queries.append(prompt)

    # –û—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    for word in words:
        if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
            search_queries.append(word)

    # –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Å–ª–æ–≤
    if len(words) > 1:
        for i in range(len(words)-1):
            search_queries.append(f"{words[i]} {words[i+1]}")

    # 3. –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    all_downloaded_images = []

    for query in search_queries[:2]:  # –£–º–µ–Ω—å—à–∏–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        print(f"üîç –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è: '{query}'")
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ –∑–∞–ø—Ä–æ—Å
        downloaded = download_real_images(query, num_images=3)
        all_downloaded_images.extend(downloaded)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—É–∑—É –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        if downloaded:
            print(f"‚è∏Ô∏è –ü–∞—É–∑–∞ 2 —Å–µ–∫ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏...")
            time.sleep(2)

        if len(all_downloaded_images) >= max_images:
            break

    print(f"üì• –ò—Ç–æ–≥–æ —Å–∫–∞—á–∞–Ω–æ {len(all_downloaded_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    generated_image = None

    if all_downloaded_images:
        print("üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        generated_image = generate_from_dataset(all_downloaded_images, prompt)

    # 5. –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–º–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    if not generated_image:
        print("üé≠ Fallback: –∏—Å–ø–æ–ª—å–∑—É—é —É–º–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
        smart_images = generate_smart_base_images(prompt, num_images=1)
        generated_image = smart_images[0] if smart_images else None

    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    generation_data = {
        'prompt': prompt,
        'tokens': tokens,
        'words': words,
        'search_queries': search_queries,
        'downloaded_images': all_downloaded_images,
        'generated_image': generated_image,
        'timestamp': time.time(),
        'method': 'intelligent_dataset' if all_downloaded_images else 'smart_fallback'
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    data_file = 'intelligent_generation_data.json'
    if os.path.exists(data_file):
        with open(data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    else:
        data = []

    data.append(generation_data)

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if len(data) > 100:
        data = data[-100:]

    with open(data_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {data_file}")

    return generated_image

def generate_image_from_text(prompt, bpe, max_images=5):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å—é"""
    try:
        print(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è: '{prompt}'")
        start_time = time.time()

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        generated_image = None
        
        # –®–∞–≥ 1: –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ
        print("üåê –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        real_images = download_real_images(prompt, num_images=3)
        
        if real_images:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(real_images)} —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            generated_image = generate_from_dataset(real_images, prompt)
            if generated_image:
                print("‚úÖ –°–æ–∑–¥–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –®–∞–≥ 2: –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        if not generated_image:
            print("üß† –ò—Å–ø–æ–ª—å–∑—É—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...")
            generated_image = intelligent_image_generation(prompt, bpe, max_images)
        
        # –®–∞–≥ 3: Fallback –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—É—é —É–º–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        if not generated_image:
            print("üé≠ Fallback: —É–º–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            smart_images = generate_smart_base_images(prompt, num_images=1)
            generated_image = smart_images[0] if smart_images else None

        generation_time = time.time() - start_time
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generation_time:.2f} —Å–µ–∫—É–Ω–¥")

        if generated_image:
            print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {generated_image}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if os.path.exists(generated_image):
                file_size = os.path.getsize(generated_image)
                print(f"üìÅ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size} –±–∞–π—Ç")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º RLHF –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                try:
                    category = improve_unknown_word_generation(prompt) or 'default'
                    quality = evaluate_image_quality(generated_image)
                    auto_score = quality.get('auto_score', 0.5)
                    
                    # –ï—Å–ª–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –≤—ã—Å–æ–∫–∞—è, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                    if auto_score > 0.7:
                        generation_params = {
                            'method': 'dataset' if real_images else 'intelligent',
                            'real_images_count': len(real_images) if real_images else 0,
                            'prompt': prompt,
                            'auto_score': auto_score
                        }
                        reinforcement_learning_update(category, auto_score, generation_params)
                        print(f"üß† RLHF: –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è '{category}' (–æ—Ü–µ–Ω–∫–∞: {auto_score:.3f})")
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ RLHF: {e}")
                
                return generated_image
            else:
                print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {generated_image}")
                return None
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            return None

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return None

def improve_unknown_word_generation(query):
    """–£–ª—É—á—à–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤"""
    translate_dict = {
        '–Ω–µ–±–æ': 'sky',
        '–º–æ—Ä–µ': 'ocean',
        '–ª–µ—Å': 'forest',
        '—Å–æ–ª–Ω—Ü–µ': 'sun',
        '–∫–æ—Ç': 'cat',
        '—Å–æ–±–∞–∫–∞': 'dog',
        '–æ–≥–æ–Ω—å': 'fire',
        '–∫–æ—Å–º–æ—Å': 'space',
        '–∑–¥–∞–Ω–∏–µ': 'building',
        '–º–∞—à–∏–Ω–∞': 'transport',
        '–µ–¥–∞': 'food',
    }
    for ru_word, en_word in translate_dict.items():
        if ru_word in query.lower():
            return en_word
    return None

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è RLHF
def evaluate_image_quality(image_path, user_rating=None):
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        # –ü—Ä–æ—Å—Ç–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ –∏ —è—Ä–∫–æ—Å—Ç–∏
        with Image.open(image_path) as img:
            img_gray = img.convert('L')
            pixels = list(img_gray.getdata())
            
            # –ö–æ–Ω—Ç—Ä–∞—Å—Ç
            contrast = (max(pixels) - min(pixels)) / 255.0 if pixels else 0
            
            # –Ø—Ä–∫–æ—Å—Ç—å
            brightness = sum(pixels) / (len(pixels) * 255.0) if pixels else 0
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            auto_score = (contrast * 0.3 + brightness * 0.7)
            
            return {
                'auto_score': auto_score,
                'user_rating': user_rating,
                'contrast': contrast,
                'brightness': brightness
            }
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        return {'auto_score': 0.5, 'user_rating': user_rating}

def reinforcement_learning_update(category, reward, generation_params):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ RLHF –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö.
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –±–∞–∑—ã
        rows = db.select_from('improved_generation_params', where='category = ?', params=(category,))
        import json
        if rows:
            data = rows[0]
            scores = json.loads(data['scores'])
            scores.append(reward)
            if len(scores) > 50:
                scores = scores[-50:]
            avg_score = sum(scores) / len(scores)
            best_params = json.loads(data['best_params'])
            if reward > avg_score:
                best_params = generation_params.copy()
            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å
            db.update('improved_generation_params',
                      where='category = ?',
                      where_params=(category,),
                      scores=json.dumps(scores, ensure_ascii=False),
                      avg_score=avg_score,
                      best_params=json.dumps(best_params, ensure_ascii=False),
                      total_generations=data['total_generations'] + 1)
        else:
            # –í—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            db.insert_into('improved_generation_params',
                           category=category,
                           scores=json.dumps([reward], ensure_ascii=False),
                           avg_score=reward,
                           best_params=json.dumps(generation_params, ensure_ascii=False),
                           total_generations=1)
        print(f"RLHF: –û–±–Ω–æ–≤–ª–µ–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è '{category}' —Å –Ω–æ–≤–æ–π –æ—Ü–µ–Ω–∫–æ–π {reward:.3f}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è RLHF –≤ –ë–î: {e}")

load_text_rlhf_policy()
load_image_rlhf_policy()